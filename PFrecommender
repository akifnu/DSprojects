{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3PjppsG+27eQQ1TIpbeQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akifnu/DSprojects/blob/main/PFrecommender\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟢 Perfume Recommender: A Journey from Memory to Scent\n",
        "\n",
        "This tool began as an experience during many visits to perfume shops around the world. I love perfumes and I travel. I would watch as customers came in, not searching for a specific bottle or brand, but instead sharing a memory, a feeling, or a fleeting emotion they connected with a scent. Some spoke of a summer by the sea, others of a sweet party fragrance, and some even asked for a perfume that captured the excitement of new beginnings. Often, the shop staff seemed rattled by these requests, bringing out bottle after bottle for the customer to test—sometimes turning what should be a joyful discovery into a long struggle.\n",
        "\n",
        "These moments made me realize that perfume is so much more than a product; it’s an invisible link to our deepest feelings. I saw how artificial intelligence could help small shops quickly retrieve the perfumes that truly match their clients’ stories and emotions. With this tool, my hope is to bridge the gap between memory and scent, making the journey to the perfect fragrance easier and more meaningful for everyone.\n",
        "\n",
        "To try it yourself, clone the repo with:\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/your-username/perfume-recommender.git\n",
        "cd perfume-recommender\n"
      ],
      "metadata": {
        "id": "JvX8YgJ3nYRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Environment Setup and Package Installation\n",
        "\n",
        "Before running the main perfume recommender tool, it’s important to make sure your Python environment is clean and has the right libraries. This code block first removes any older or possibly conflicting packages that could cause errors. It then installs only the necessary tools for this notebook to work with AI, data, and perfume matching.\n",
        "\n",
        "- **PyTorch and torchvision** are installed with GPU support, optimized for fast AI processing on Google Colab’s T4 GPUs.\n",
        "- **sentence-transformers** lets the system use advanced AI to compare your perfume request with the database.\n",
        "- **faiss-gpu** is chosen (instead of faiss-cpu) to enable fast, large-scale similarity searches using your GPU.\n",
        "- **langchain-community** and **aisuite** are installed to help manage AI prompts and API calls.\n",
        "- **openpyxl** is needed to read Excel files containing perfume data.\n",
        "- **groq** is installed for access to the Groq API, which runs large language models for smart recommendations.\n",
        "\n",
        "By running this cell, you prepare your environment for the best performance, stability, and compatibility—whether you’re a casual user or a developer. No manual setup required!\n"
      ],
      "metadata": {
        "id": "0CvKJSrlmgn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCbYA87QSKNe",
        "outputId": "fd0e00d7-6350-4d4c-efd0-3e4c6a90687e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu124\n",
            "Uninstalling torch-2.5.1+cu124:\n",
            "  Successfully uninstalled torch-2.5.1+cu124\n",
            "Found existing installation: torchvision 0.20.1+cu124\n",
            "Uninstalling torchvision-0.20.1+cu124:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu124\n",
            "Found existing installation: sentence-transformers 3.4.1\n",
            "Uninstalling sentence-transformers-3.4.1:\n",
            "  Successfully uninstalled sentence-transformers-3.4.1\n",
            "\u001b[33mWARNING: Skipping faiss-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping aisuite as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m745.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.7.0+cu118 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.7.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers) (75.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-4.1.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.25 (from langchain-community)\n",
            "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: typing-inspection, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.40\n",
            "    Uninstalling langchain-core-0.3.40:\n",
            "      Successfully uninstalled langchain-core-0.3.40\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.6\n",
            "    Uninstalling langchain-text-splitters-0.3.6:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.19\n",
            "    Uninstalling langchain-0.3.19:\n",
            "      Successfully uninstalled langchain-0.3.19\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.25 langchain-community-0.3.24 langchain-core-0.3.60 langchain-text-splitters-0.3.8 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 typing-inspection-0.4.0\n",
            "Collecting aisuite\n",
            "  Downloading aisuite-0.1.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from aisuite)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite) (0.14.0)\n",
            "Downloading aisuite-0.1.11-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx, aisuite\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed aisuite-0.1.11 httpx-0.27.2\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Clean uninstall of possibly conflicting packages\n",
        "!pip uninstall -y torch torchvision sentence-transformers faiss-cpu langchain-community aisuite\n",
        "\n",
        "# Install only necessary libraries\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118  # GPU version for T4 [^1]\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-gpu   # Use faiss-gpu for T4, not faiss-cpu [^2]\n",
        "!pip install langchain-community\n",
        "!pip install aisuite\n",
        "!pip install openpyxl    # For pd.read_excel\n",
        "!pip install groq\n",
        "# [^1]: https://pytorch.org/get-started/locally/\n",
        "# [^2]: https://github.com/facebookresearch/faiss/blob/main/INSTALL.md\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-community faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3SRp-DbTSkh",
        "outputId": "90db566a-5cbf-4b27-d3ea-91d3dd7e20c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟢 Perfume Recommender — How It Works\n",
        "\n",
        "This tool is designed to help anyone—whether you know code or not—discover perfumes that fit your style. You simply enter a description of your ideal scent (such as “fresh, green, and affordable”), and the system handles the rest.\n",
        "\n",
        "First, the tool loads a large perfume database and applies filters to focus on popular and well-reviewed options. It uses advanced AI called sentence embeddings to understand both your words and each perfume’s description in a “semantic” way, meaning it goes beyond keyword matching to understand your intent and the perfume’s real qualities.\n",
        "\n",
        "The system then searches for perfumes with features closest to your description, using fast similarity search algorithms (FAISS). From these, the top 25 matches are selected.\n",
        "\n",
        "What makes this tool unique is its AI-powered ranking step: five virtual expert agents, each with a different perspective (literal, emotional, contextual, conflict resolution, and overall synthesis), analyze the candidates and come to a consensus on the best five to show you.\n",
        "\n",
        "All the technical parts—data handling, AI language models, and voting logic—run automatically in the background. You simply get clear, expert-picked recommendations, but anyone interested can see and follow the machine learning and reasoning process behind the results.\n"
      ],
      "metadata": {
        "id": "Ztt0TnIfl3h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import aisuite as ai\n",
        "import torch\n",
        "\n",
        "# --- 1. API Key (secure input) ---\n",
        "os.environ['GROQ_API_KEY'] = getpass.getpass('Enter your GROQ API key: ')\n",
        "\n",
        "# --- 2. Load & Filter Data (CLEAN) ---\n",
        "df = pd.read_excel('/content/Fragrantica.xlsx')\n",
        "df.columns = df.columns.str.strip()  # Remove accidental whitespace\n",
        "\n",
        "brand_counts = df[\"Brand\"].value_counts()\n",
        "brands_to_keep = brand_counts[brand_counts >= 60].index\n",
        "df_filtered = df[df[\"Brand\"].isin(brands_to_keep)].copy()\n",
        "\n",
        "df_filtered = df_filtered.dropna(subset=[\"Year\"])\n",
        "df_filtered[\"Year\"] = df_filtered[\"Year\"].astype(int)\n",
        "df_filtered = df_filtered[df_filtered[\"Year\"] >= 2000]\n",
        "\n",
        "df_filtered[\"Rating Value\"] = (\n",
        "    df_filtered[\"Rating Value\"]\n",
        "    .astype(str)\n",
        "    .str.replace(\",\", \".\", regex=False)\n",
        "    .astype(float)\n",
        ")\n",
        "df_filtered = df_filtered[df_filtered[\"Rating Value\"] >= 4]\n",
        "df_filtered = df_filtered.reset_index(drop=True)\n",
        "\n",
        "# --- 3. Add Description Column Consistently ---\n",
        "def combine_notes_accord(row):\n",
        "    notes = row.get('Notes', 'N/A')\n",
        "    accord = row.get('Accord', 'N/A')\n",
        "    brand = row.get('Brand', 'N/A')\n",
        "    gender = row.get(\"Gender\", \"N/A\")\n",
        "    return f\"Notes: {notes}; Accord: {accord}; Brand: {brand}; Perfume's Gender (unisex should be both men and women): {gender}\"\n",
        "\n",
        "df_filtered['Description'] = df_filtered.apply(combine_notes_accord, axis=1)\n",
        "\n",
        "# --- 4. Embedding Model + FAISS Index (with caching) ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\", trust_remote_code=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# --- FAISS/Numpy cache paths ---\n",
        "EMBEDDINGS_PATH = \"perfume_embeddings.npy\"\n",
        "FAISS_INDEX_PATH = \"faiss_index.bin\"\n",
        "descriptions = df_filtered['Description'].tolist()\n",
        "\n",
        "def embeddings_exist(emb_path, index_path):\n",
        "    return os.path.exists(emb_path) and os.path.exists(index_path)\n",
        "\n",
        "def save_embeddings_and_index(embeddings, index, emb_path, index_path):\n",
        "    np.save(emb_path, embeddings)\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "def load_embeddings_and_index(emb_path, index_path):\n",
        "    embeddings = np.load(emb_path)\n",
        "    index = faiss.read_index(index_path)\n",
        "    return embeddings, index\n",
        "\n",
        "if embeddings_exist(EMBEDDINGS_PATH, FAISS_INDEX_PATH):\n",
        "    print(\"Loading embeddings and FAISS index from disk...\")\n",
        "    doc_embeddings, index = load_embeddings_and_index(EMBEDDINGS_PATH, FAISS_INDEX_PATH)\n",
        "else:\n",
        "    print(\"No cached embeddings/index found. Creating new ones...\")\n",
        "    doc_embeddings = model.encode(descriptions, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    doc_embeddings = np.array(doc_embeddings, dtype='float32')\n",
        "    d = doc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(doc_embeddings)\n",
        "    save_embeddings_and_index(doc_embeddings, index, EMBEDDINGS_PATH, FAISS_INDEX_PATH)\n",
        "\n",
        "# --- 5. Retrieval Function: strictly 25 candidates ---\n",
        "def recommend_perfumes(user_input, top_n=25):\n",
        "    query_embedding = model.encode([user_input], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(query_embedding, top_n)\n",
        "    recommendations = df_filtered.iloc[I[0]].copy()\n",
        "    recommendations = recommendations.reset_index(drop=True)\n",
        "    recommendations['Distance'] = D[0]\n",
        "    return recommendations\n",
        "\n",
        "# --- 6. Layered Agent Panel Reranker ---\n",
        "def rerank_with_deepseek(user_query, candidates_df, final_n=5):\n",
        "    prompt = (\n",
        "        f\"The user is searching for: \\\"{user_query}\\\"\\n\\n\"\n",
        "        \"Here are 25 candidate perfumes. For each, all known information is provided:\\n\"\n",
        "    )\n",
        "    for i, row in enumerate(candidates_df.itertuples(), 1):\n",
        "        prompt += f\"{i}. \" + \", \".join([f\"{col}: {getattr(row, col, 'N/A')}\" for col in candidates_df.columns]) + \"\\n\"\n",
        "\n",
        "    prompt += (\n",
        "        \"\\nYou are a panel of 5 fragrance experts, each bringing a unique perspective:\"\n",
        "        \"\\n- Literal: Focuses on explicit user requirements.\"\n",
        "        \"\\n- Emotional: Detects the desired mood or vibe.\"\n",
        "        \"\\n- Contextual: Considers season, occasion, or lifestyle, even if implied.\"\n",
        "        \"\\n- Resolver: Reconciles conflicting or ambiguous user requests.\"\n",
        "        \"\\n- Synthesizer: Integrates all viewpoints for the best overall fit.\"\n",
        "        \"\\nEach agent selects and ranks their top 5 candidates (privately).\"\n",
        "        \" The panel agrees on a single consensus ranking of exactly 5 perfumes that best fit the user's intent.\"\n",
        "        \"\\nOutput ONLY the final consensus: 5 candidate numbers (1-based), one per line. No explanations.\"\n",
        "    )\n",
        "    client = ai.Client()\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a panel of 5 expert perfumers. Output only the consensus top 5, no explanations.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"groq:meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- 7. Parse Reranker Output (guarantee only top 5) ---\n",
        "def parse_and_reorder(rerank_result, candidates_df, final_n=5):\n",
        "    numbers = re.findall(r'\\d+', rerank_result)\n",
        "    if not numbers:\n",
        "        print(\"Could not parse rerank output!\")\n",
        "        return candidates_df.head(final_n)\n",
        "    indices = [int(n)-1 for n in numbers if int(n) <= len(candidates_df)]\n",
        "    indices = indices[:final_n]\n",
        "    reranked_df = candidates_df.iloc[indices]\n",
        "    return reranked_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKres9ZjSLpr",
        "outputId": "e7efdc0e-1e01-4cce-849d-54f5f8b246fe"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GROQ API key: ··········\n",
            "Using device: cuda\n",
            "Loading embeddings and FAISS index from disk...\n",
            "Describe your ideal perfume: green fresh and cheap\n",
            "\n",
            "Top 5 recommendations:\n",
            "\n",
            "      id                                                url  \\\n",
            "0  12198  https://www.fragrantica.com/perfume/natura/gen...   \n",
            "1  19809  https://www.fragrantica.com/perfume/zara/carda...   \n",
            "2  15960  https://www.fragrantica.com/perfume/kenzo/toky...   \n",
            "3  13584  https://www.fragrantica.com/perfume/givenchy/n...   \n",
            "4  15886  https://www.fragrantica.com/perfume/avon/exclu...   \n",
            "\n",
            "                   Perfume     Brand Country  Gender  Rating Value  \\\n",
            "0                      gen    natura  Brazil     men          4.00   \n",
            "1  cardamom gender neutral      zara   Spain  unisex          4.26   \n",
            "2           tokyo by kenzo     kenzo  France     men          4.11   \n",
            "3               noctambule  givenchy  France  unisex          4.04   \n",
            "4        exclusive reserve      avon     USA     men          4.11   \n",
            "\n",
            "   Rating Count  Year                                              Notes  \\\n",
            "0            41  2003  Top notes: green pepper, pink pepper, black pe...   \n",
            "1           199  2018  Top notes: guatemalan cardamom; Middle notes: ...   \n",
            "2           905  2007  Top notes: ginger, grapefruit, lemon; Middle n...   \n",
            "3            96  2022  Top notes: grasse rose, pink pepper, cumin; Mi...   \n",
            "4            44  2019  Top notes: cardamom, pepper; Middle notes: ger...   \n",
            "\n",
            "                                        Accord  \\\n",
            "0  warm spicy,green,fresh spicy,woody,aromatic   \n",
            "1                    warm spicy,green,aromatic   \n",
            "2    citrus,woody,warm spicy,fresh spicy,fresh   \n",
            "3       rose,oud,fresh spicy,floral,soft spicy   \n",
            "4  aromatic,fresh spicy,warm spicy,green,woody   \n",
            "\n",
            "                                         Description  Distance  \n",
            "0  Notes: Top notes: green pepper, pink pepper, b...  0.805523  \n",
            "1  Notes: Top notes: guatemalan cardamom; Middle ...  0.803424  \n",
            "2  Notes: Top notes: ginger, grapefruit, lemon; M...  0.802158  \n",
            "3  Notes: Top notes: grasse rose, pink pepper, cu...  0.801312  \n",
            "4  Notes: Top notes: cardamom, pepper; Middle not...  0.800771  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "kHKSNbnvf2um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟢 How to Use the Perfume Recommender\n",
        "\n",
        "When you run this code, you’ll be asked to type a short description of your ideal perfume—for example, “fresh and green for summer” or “sweet and affordable for women.” Once you press enter, the system uses machine learning to search a large perfume dataset for matches that best fit your request.\n",
        "\n",
        "First, the program translates your input and every perfume’s features into a special language format using powerful AI embeddings. It then compares your query to all perfumes in the database, rapidly finding the top 25 most similar scents with advanced search algorithms.\n",
        "\n",
        "Next, the list of candidates is sent to a team of five virtual “expert agents.” Each agent\n"
      ],
      "metadata": {
        "id": "Umm_E008maGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    user_query = input(\"Describe your ideal perfume: \").strip()\n",
        "    final_n = 5\n",
        "    candidates = recommend_perfumes(user_query, top_n=25)  # STRICTLY 25\n",
        "    if candidates.empty:\n",
        "        print(\"No matching perfumes found.\")\n",
        "    else:\n",
        "        rerank_result = rerank_with_deepseek(user_query, candidates, final_n=final_n)\n",
        "        reranked = parse_and_reorder(rerank_result, candidates, final_n=final_n)\n",
        "        print(\"\\nTop 5 recommendations:\\n\")\n",
        "        print(reranked.reset_index(drop=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vk_MGtPdUXI",
        "outputId": "20465c59-04e4-4ed5-ac73-8927c3b808b4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe your ideal perfume: Bring me this is her from zadig and voltaire\n",
            "\n",
            "Top 5 recommendations:\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [id, url, Perfume, Brand, Country, Gender, Rating Value, Rating Count, Year, Notes, Accord, Description, Distance]\n",
            "Index: []\n"
          ]
        }
      ]
    }
  ]
}